{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcasivaram/Datascience/blob/main/Null_Innovation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NULL Innovation Private Limited**"
      ],
      "metadata": {
        "id": "V8F7SmjkQc_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task:**\n",
        "# 1. Identify the intent of tweet using ML models."
      ],
      "metadata": {
        "id": "_Ap8buZWRPLN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5uIMiHU6vIJ9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4smLVXrkwcn3"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv(\"/content/drive/MyDrive/Tweet_NFT.xlsx - Sheet1.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the dataset :"
      ],
      "metadata": {
        "id": "divXpEWMRgUF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6zinKIkXxynO"
      },
      "outputs": [],
      "source": [
        "df1=pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oyoq6l1R5PhF",
        "outputId": "e567a6d0-6e6f-407c-d02a-1a16ff9acfdd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96364"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df1['tweet_intent'].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6VsMmYUysfq",
        "outputId": "91104943-b6d8-4129-ab2d-f9cfd36cf43f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31089"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df1['tweet_intent'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rRI5z_sQLpEC",
        "outputId": "45fe28fa-5f70-4956-963e-52b8559a57f9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                         tweet_text  \\\n",
              "0  1212762  @crypto_brody @eCoLoGy1990 @MoonrunnersNFT @It...   \n",
              "1  1212763  Need Sick Character artâ“#art #artist #Artist...   \n",
              "2  1212765  @The_Hulk_NFT @INagotchiNFT @Tesla @killabears...   \n",
              "3  1212766  @CryptoBatzNFT @DarekBTW The first project in ...   \n",
              "4  1212767  @sashadysonn The first project in crypto with ...   \n",
              "\n",
              "           tweet_created_at  tweet_intent  \n",
              "0  2022-08-06T16:56:36.000Z     Community  \n",
              "1  2022-08-06T16:56:36.000Z      Giveaway  \n",
              "2  2022-08-06T16:56:35.000Z  Appreciation  \n",
              "3  2022-08-06T16:56:35.000Z     Community  \n",
              "4  2022-08-06T16:56:34.000Z     Community  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-61dc8aaf-eb78-4bab-b63e-5f0be3a927c7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>tweet_created_at</th>\n",
              "      <th>tweet_intent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1212762</td>\n",
              "      <td>@crypto_brody @eCoLoGy1990 @MoonrunnersNFT @It...</td>\n",
              "      <td>2022-08-06T16:56:36.000Z</td>\n",
              "      <td>Community</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1212763</td>\n",
              "      <td>Need Sick Character artâ“#art #artist #Artist...</td>\n",
              "      <td>2022-08-06T16:56:36.000Z</td>\n",
              "      <td>Giveaway</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1212765</td>\n",
              "      <td>@The_Hulk_NFT @INagotchiNFT @Tesla @killabears...</td>\n",
              "      <td>2022-08-06T16:56:35.000Z</td>\n",
              "      <td>Appreciation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1212766</td>\n",
              "      <td>@CryptoBatzNFT @DarekBTW The first project in ...</td>\n",
              "      <td>2022-08-06T16:56:35.000Z</td>\n",
              "      <td>Community</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1212767</td>\n",
              "      <td>@sashadysonn The first project in crypto with ...</td>\n",
              "      <td>2022-08-06T16:56:34.000Z</td>\n",
              "      <td>Community</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61dc8aaf-eb78-4bab-b63e-5f0be3a927c7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-61dc8aaf-eb78-4bab-b63e-5f0be3a927c7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-61dc8aaf-eb78-4bab-b63e-5f0be3a927c7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Droping unwanted columns :"
      ],
      "metadata": {
        "id": "FdYypigGRwfz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hIfKHhiB1cjJ"
      },
      "outputs": [],
      "source": [
        "df1=df1.dropna()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TpkqNpWKzRWu"
      },
      "outputs": [],
      "source": [
        "df1=df1.drop(['id','tweet_created_at'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5GqfZ6kLjGY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYHXdARPWagi",
        "outputId": "3112ddda-fabf-48d9-da4b-a441e6a5550a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        @crypto_brody @eCoLoGy1990 @MoonrunnersNFT @It...\n",
              "1        Need Sick Character artâ“#art #artist #Artist...\n",
              "2        @The_Hulk_NFT @INagotchiNFT @Tesla @killabears...\n",
              "3        @CryptoBatzNFT @DarekBTW The first project in ...\n",
              "4        @sashadysonn The first project in crypto with ...\n",
              "                               ...                        \n",
              "96359    @nft_cryptogang @liuyan93721534 0x9fE808D8a9E2...\n",
              "96360    Just registered to win @cryptopunksnfts #1859 ...\n",
              "96361    @SolSniffer Dyor on the nft collection \"Dictat...\n",
              "96362    #NFT #NFTCommmunity #DeltaFlare\\nJoin me on Di...\n",
              "96363    @DuckBathing @diozoth @elonmusk @richerd @John...\n",
              "Name: tweet_text, Length: 96364, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df1['tweet_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starts cleaning the data :"
      ],
      "metadata": {
        "id": "QcNRiIlQR5MP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5wFkiWEBwAH",
        "outputId": "10129267-eb7c-4097-98f9-4838bece74e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ],
      "source": [
        "df1['tweet_processed'] = df1['tweet_text'].str.replace(\"[^a-zA-Z]\", \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IE3tpipB0Nak"
      },
      "outputs": [],
      "source": [
        "df1['tweet_processed'] = df1['tweet_processed'].apply(lambda row: ' '.join([word for word in row.split() if len(word)>2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdUi3699WVco",
        "outputId": "91ea7dcb-1573-4673-c47c-2935bc83ab99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        crypto brody eCoLoGy MoonrunnersNFT ItsukiNFT ...\n",
              "1        Need Sick Character art art artist Artists ani...\n",
              "2        The Hulk NFT INagotchiNFT Tesla killabearsnft ...\n",
              "3        CryptoBatzNFT DarekBTW The first project crypt...\n",
              "4        sashadysonn The first project crypto with move...\n",
              "                               ...                        \n",
              "96359                     nft cryptogang liuyan Thanks lot\n",
              "96360    Just registered win cryptopunksnfts with blokp...\n",
              "96361    SolSniffer Dyor the nft collection Dictators t...\n",
              "96362    NFT NFTCommmunity DeltaFlare Join Discord http...\n",
              "96363    DuckBathing diozoth elonmusk richerd John NFT ...\n",
              "Name: tweet_processed, Length: 96364, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df1['tweet_processed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lurdqOaYWwSP"
      },
      "outputs": [],
      "source": [
        "df1['tweet_processed'] = [tweet.lower() for tweet in df1['tweet_processed']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlz_suv2W98l",
        "outputId": "231cea71-e5f2-40b7-a930-64395afb6e8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        crypto brody ecology moonrunnersnft itsukinft ...\n",
              "1        need sick character art art artist artists ani...\n",
              "2        the hulk nft inagotchinft tesla killabearsnft ...\n",
              "3        cryptobatznft darekbtw the first project crypt...\n",
              "4        sashadysonn the first project crypto with move...\n",
              "                               ...                        \n",
              "96359                     nft cryptogang liuyan thanks lot\n",
              "96360    just registered win cryptopunksnfts with blokp...\n",
              "96361    solsniffer dyor the nft collection dictators t...\n",
              "96362    nft nftcommmunity deltaflare join discord http...\n",
              "96363    duckbathing diozoth elonmusk richerd john nft ...\n",
              "Name: tweet_processed, Length: 96364, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "df1['tweet_processed']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Word Tokenize and StopWords Removal : "
      ],
      "metadata": {
        "id": "0zPKzWnySJAr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkDBgasQXQiy",
        "outputId": "819dd3cf-91dc-4ae3-86b3-23af2022d3da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "stop_words = stopwords.words('english')\n",
        "# Function to remove stop words \n",
        "def remove_stopwords(tweet):\n",
        "    # iNPUT : IT WILL TAKE ROW/REVIEW AS AN INPUT\n",
        "    # take the paragraph, break into words, check if the word is a stop word, remove if stop word, combine the words into a para again\n",
        "    tweet_tokenized = word_tokenize(tweet)\n",
        "    tweet_new = \" \".join([i for i in tweet_tokenized  if i not in stop_words])\n",
        "    return tweet_new\n",
        "df1['tweet_processed'] = [remove_stopwords(tweet) for tweet in df1['tweet_processed']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l1buDTrDA-D",
        "outputId": "ebab1ca7-b7fa-481b-81be-65debed99f68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        crypto brody ecology moonrunnersnft itsukinft ...\n",
              "1        need sick character art art artist artists ani...\n",
              "2        hulk nft inagotchinft tesla killabearsnft elon...\n",
              "3        cryptobatznft darekbtw first project crypto mo...\n",
              "4        sashadysonn first project crypto move earn ast...\n",
              "                               ...                        \n",
              "96359                     nft cryptogang liuyan thanks lot\n",
              "96360    registered win cryptopunksnfts blokpax let fre...\n",
              "96361    solsniffer dyor nft collection dictators build...\n",
              "96362    nft nftcommmunity deltaflare join discord http...\n",
              "96363    duckbathing diozoth elonmusk richerd john nft ...\n",
              "Name: tweet_processed, Length: 96364, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "df1['tweet_processed']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Lemmatizer Function :"
      ],
      "metadata": {
        "id": "GISYMuemSY8w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lebVgkEulBFs",
        "outputId": "caef3115-cfbc-4094-c6ed-f7243ba3541b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# function to convert nltk tag to wordnet tag\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Finds the part of speech tag\n",
        "# Convert the detailed POS tag into a shallow information\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "\n",
        "# lemmatize sentence using pos tag\n",
        "def lemmatize_sentence(sentence):\n",
        "  # word tokenize -> pos tag (detailed) -> wordnet tag (shallow pos) -> lemmatizer -> root word\n",
        "    #tokenize the sentence and find the POS tag for each token\n",
        "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
        "    #tuple of (token, wordnet_tag)\n",
        "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in wordnet_tagged:\n",
        "        if tag is None:\n",
        "            #if there is no available tag, append the token as is\n",
        "            lemmatized_sentence.append(word)\n",
        "        else:        \n",
        "            #else use the tag to lemmatize the token\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "    return \" \".join(lemmatized_sentence)\n",
        "\n",
        "\n",
        "df1['tweet_processed'] = df1['tweet_processed'].apply(lambda x: lemmatize_sentence(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwvTcMpOEGLo",
        "outputId": "a6d557be-13d2-472b-c7bc-5aaa28b3233e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Community', 'Giveaway', 'Appreciation', 'Presale', 'Whitelist',\n",
              "       'pinksale', 'Done', 'Interested', 'Launching Soon'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "df1['tweet_intent'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using TF-IDF Function :\n",
        "# Encoding and Splitting the Data."
      ],
      "metadata": {
        "id": "IM_ZrFDySgsU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9LXsaS66nNk",
        "outputId": "b77eac9e-5388-48d5-e4d2-0b8098e9b28b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# Importing module\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import sklearn\n",
        "# Creating matrix of top 2500 tokens\n",
        "tfidf = TfidfVectorizer(max_features=2500)\n",
        " \n",
        "\n",
        "X = tfidf.fit_transform(df1.tweet_processed).toarray()\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "y = df1['tweet_intent'].map({'Community':0,'Giveaway':1, 'Appreciation':2, 'Presale':3, 'Whitelist':4,'pinksale':5, 'Done':6, 'Interested':7, 'Launching Soon':8})\n",
        "featureNames = tfidf.get_feature_names()\n",
        "\n",
        "\n",
        "X, y\n",
        "\n",
        "\n",
        "# Splitting the dataset into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRz3Yju_GaJQ",
        "outputId": "b57c2490-c676-4017-c2c8-5114ac684d12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W59PxiJGcCq",
        "outputId": "6184891a-b951-4858-e54c-ac4075782a29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "y.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHIVlZhgGpZ0",
        "outputId": "31567fb3-0630-4e9d-c289-887e49923770"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((77091, 2500), (19273, 2500))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Gaussian Naive Bayes model :"
      ],
      "metadata": {
        "id": "z-Dnl_C5SqYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model on training set\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "UFXnDmFLxbyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7244459f-7cb8-4c5b-9b4c-2459ed78b32d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB()"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making predictions on the testing set\n",
        "y_pred = gnb.predict(X_test)"
      ],
      "metadata": {
        "id": "PlXOJSE8xcv1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comparing actual response values (y_test) with predicted response values (y_pred)\n",
        "from sklearn import metrics\n",
        "print(\"Gaussian Naive Bayes model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)"
      ],
      "metadata": {
        "id": "bZHwKd2cxf1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OUTPUT :\n",
        "# Gaussian Naive Bayes model accuracy(in %): **65.6306750376174**"
      ],
      "metadata": {
        "id": "AdMtjxudTDsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Logistic Regression Model :"
      ],
      "metadata": {
        "id": "nauHhO8HS5Us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression(random_state = 0)\n",
        "log_reg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "IjlZiFFgxjFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = log_reg.predict(X_test)"
      ],
      "metadata": {
        "id": "yluJpuCYxpAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_test_set = log_reg.score(X_test, y_test)\n",
        "accuracy_test_set"
      ],
      "metadata": {
        "id": "dFySJc5xyQgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"{0:.2f}\".format(round(accuracy_test_set, 2)))"
      ],
      "metadata": {
        "id": "fGNVQ0ivySV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "id": "5v-PcFLX9DQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc_score(y_test, log_reg.predict_proba(X_test),multi_class=\"ovr\")"
      ],
      "metadata": {
        "id": "qDHVabXxyqlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "  \n",
        "print (\"Confusion Matrix : \\n\", cm)"
      ],
      "metadata": {
        "id": "3aH1VZNzxreP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OUTPUT :**\n",
        "# Logistic Regression model accuracy: **0.9959990208680227**\n",
        "# **Conclusion :**\n",
        "# Logistic Regression model will gives higher accuracy so we select this model for final predictions."
      ],
      "metadata": {
        "id": "FnhsXOoeTd_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Test Dataset to filling the missed Intents :"
      ],
      "metadata": {
        "id": "VCHJu2W_T1gf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Dataset :"
      ],
      "metadata": {
        "id": "_BJlKyChUpJV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rupenVe7-6Vb"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv(\"/content/drive/MyDrive/Tweet_NFT.xlsx - Sheet1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-LG9aEI-6Vd"
      },
      "outputs": [],
      "source": [
        "df2=pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "id": "41wbGfIL_NtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2=df2.iloc[96365:,[1,3]]"
      ],
      "metadata": {
        "id": "scFao93Y_KLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "id": "S4rS90gZ_fD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zkf6rez--6Vd"
      },
      "outputs": [],
      "source": [
        "df2['tweet_intent'].count()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starts Cleaning :"
      ],
      "metadata": {
        "id": "wasonyf0Uu4s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovrzVG7J-6Ve"
      },
      "outputs": [],
      "source": [
        "df2['tweet_processed'] = df2['tweet_text'].str.replace(\"[^a-zA-Z]\", \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2nVN1Kd-6Ve"
      },
      "outputs": [],
      "source": [
        "df2['tweet_processed'] = df2['tweet_processed'].apply(lambda row: ' '.join([word for word in row.split() if len(word)>2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BdNR4Ie-6Ve"
      },
      "outputs": [],
      "source": [
        "df2['tweet_processed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6llY4y1-6Ve"
      },
      "outputs": [],
      "source": [
        "df2['tweet_processed'] = [tweet.lower() for tweet in df2['tweet_processed']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnaunGoH-6Ve"
      },
      "outputs": [],
      "source": [
        "df2['tweet_processed']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Word Tokenizer and StopWords Removal Function :"
      ],
      "metadata": {
        "id": "fZ90aFkSVFPG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKrVHUQf-6Vf"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "stop_words = stopwords.words('english')\n",
        "# Function to remove stop words \n",
        "def remove_stopwords(tweet):\n",
        "    # iNPUT : IT WILL TAKE ROW/REVIEW AS AN INPUT\n",
        "    # take the paragraph, break into words, check if the word is a stop word, remove if stop word, combine the words into a para again\n",
        "    tweet_tokenized = word_tokenize(tweet)\n",
        "    tweet_new = \" \".join([i for i in tweet_tokenized  if i not in stop_words])\n",
        "    return tweet_new\n",
        "df2['tweet_processed'] = [remove_stopwords(tweet) for tweet in df2['tweet_processed']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQRizZzC-6Vf"
      },
      "outputs": [],
      "source": [
        "df2['tweet_processed']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Lemmatizer Function :"
      ],
      "metadata": {
        "id": "4AzoFMW6VIqc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKJ_H_li-6Vf"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# function to convert nltk tag to wordnet tag\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Finds the part of speech tag\n",
        "# Convert the detailed POS tag into a shallow information\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "\n",
        "# lemmatize sentence using pos tag\n",
        "def lemmatize_sentence(sentence):\n",
        "  # word tokenize -> pos tag (detailed) -> wordnet tag (shallow pos) -> lemmatizer -> root word\n",
        "    #tokenize the sentence and find the POS tag for each token\n",
        "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
        "    #tuple of (token, wordnet_tag)\n",
        "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in wordnet_tagged:\n",
        "        if tag is None:\n",
        "            #if there is no available tag, append the token as is\n",
        "            lemmatized_sentence.append(word)\n",
        "        else:        \n",
        "            #else use the tag to lemmatize the token\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "    return \" \".join(lemmatized_sentence)\n",
        "\n",
        "\n",
        "df2['tweet_processed'] = df2['tweet_processed'].apply(lambda x: lemmatize_sentence(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_d5-zEtK-6Vg"
      },
      "outputs": [],
      "source": [
        "df2['tweet_intent'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test1=df2.drop(\"tweet_intent\",axis=1)"
      ],
      "metadata": {
        "id": "x9FSWd-4eo4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression(random_state = 0)\n",
        "log_reg.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "vbc69OD8fF37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bEh9hYhifYJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying TF_IDF Function :"
      ],
      "metadata": {
        "id": "tGQzMk3mVRgz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhBEafj8-6Vg"
      },
      "outputs": [],
      "source": [
        "# Importing module\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import sklearn\n",
        "# Creating matrix of top 2500 tokens\n",
        "tfidf = TfidfVectorizer(max_features=2500)\n",
        " \n",
        "x_test1 = tfidf.fit_transform(df2.tweet_processed).toarray()\n",
        "\n",
        "featureNames = tfidf.get_feature_names()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting the Test Data :"
      ],
      "metadata": {
        "id": "LE_T7DJjq-JL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHG2LeZn-6Vh"
      },
      "outputs": [],
      "source": [
        "X_train.shape, x_test1.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = log_reg.predict(x_test1)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "1lRMWMo1-6Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred.shape"
      ],
      "metadata": {
        "id": "L75-ynYHjuzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.drop(\"tweet_intent\",axis=1)"
      ],
      "metadata": {
        "id": "Z7vP6C53o7Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['tweet_intent']=y_pred"
      ],
      "metadata": {
        "id": "cMWbqNfvkIDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['tweet_intent'].unique()"
      ],
      "metadata": {
        "id": "BN1t_UILkPWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reversing the Encoded Data :"
      ],
      "metadata": {
        "id": "1U43As0oqFta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2['tweet_intent']=df2['tweet_intent'].map({0:'Community',1:'Giveaway',2:'Appreciation',3:'Presale',4:'Whitelist',5:'pinksale', 6:'Done', 7:'Interested', 8:'Launching Soon'})"
      ],
      "metadata": {
        "id": "5WbXorO-ZI0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['tweet_intent'].unique()"
      ],
      "metadata": {
        "id": "qmIYXiASVkja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2"
      ],
      "metadata": {
        "id": "zzxkWkGpoftQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Output :**"
      ],
      "metadata": {
        "id": "aGr0qavyrJh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df3=pd.concat([df1,df2])\n",
        "df3"
      ],
      "metadata": {
        "id": "T4kw2X_Gn1GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Saving the updated Intents :**"
      ],
      "metadata": {
        "id": "sk6mTCYxrU0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df3.to_csv(\"Tweet_NFT_Updated.csv\",index=False) "
      ],
      "metadata": {
        "id": "0facRzH6Jeqd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "mount_file_id": "1dp130704PmyFtDu8TdvsVCFAE0qLd7uL",
      "authorship_tag": "ABX9TyMU+NS0jrLCsp+NhoYHVt3/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}