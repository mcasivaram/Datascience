{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcasivaram/Datascience/blob/main/null_innovation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uIMiHU6vIJ9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4smLVXrkwcn3"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv(\"/content/drive/MyDrive/Tweet_NFT.xlsx - Sheet1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zinKIkXxynO"
      },
      "outputs": [],
      "source": [
        "df1=pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oyoq6l1R5PhF"
      },
      "outputs": [],
      "source": [
        "df1['tweet_intent'].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6VsMmYUysfq"
      },
      "outputs": [],
      "source": [
        "df1['tweet_intent'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1=df1.dropna()  "
      ],
      "metadata": {
        "id": "hIfKHhiB1cjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpkqNpWKzRWu"
      },
      "outputs": [],
      "source": [
        "df1=df1.drop(['id','tweet_created_at'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYHXdARPWagi"
      },
      "outputs": [],
      "source": [
        "df1['tweet_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5wFkiWEBwAH"
      },
      "outputs": [],
      "source": [
        "df1['tweet_processed'] = df1['tweet_text'].str.replace(\"[^a-zA-Z]\", \" \")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1['tweet_processed'] = df1['tweet_processed'].apply(lambda row: ' '.join([word for word in row.split() if len(word)>2]))"
      ],
      "metadata": {
        "id": "IE3tpipB0Nak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdUi3699WVco"
      },
      "outputs": [],
      "source": [
        "df1['tweet_processed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lurdqOaYWwSP"
      },
      "outputs": [],
      "source": [
        "df1['tweet_processed'] = [tweet.lower() for tweet in df1['tweet_processed']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlz_suv2W98l"
      },
      "outputs": [],
      "source": [
        "df1['tweet_processed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkDBgasQXQiy"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "stop_words = stopwords.words('english')\n",
        "# Function to remove stop words \n",
        "def remove_stopwords(tweet):\n",
        "    # iNPUT : IT WILL TAKE ROW/REVIEW AS AN INPUT\n",
        "    # take the paragraph, break into words, check if the word is a stop word, remove if stop word, combine the words into a para again\n",
        "    tweet_tokenized = word_tokenize(tweet)\n",
        "    tweet_new = \" \".join([i for i in tweet_tokenized  if i not in stop_words])\n",
        "    return tweet_new\n",
        "df1['tweet_processed'] = [remove_stopwords(tweet) for tweet in df1['tweet_processed']]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1['tweet_processed']"
      ],
      "metadata": {
        "id": "6l1buDTrDA-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['tweet_processed']"
      ],
      "metadata": {
        "id": "YCAPicIRFHEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# function to convert nltk tag to wordnet tag\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Finds the part of speech tag\n",
        "# Convert the detailed POS tag into a shallow information\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "\n",
        "# lemmatize sentence using pos tag\n",
        "def lemmatize_sentence(sentence):\n",
        "  # word tokenize -> pos tag (detailed) -> wordnet tag (shallow pos) -> lemmatizer -> root word\n",
        "    #tokenize the sentence and find the POS tag for each token\n",
        "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
        "    #tuple of (token, wordnet_tag)\n",
        "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in wordnet_tagged:\n",
        "        if tag is None:\n",
        "            #if there is no available tag, append the token as is\n",
        "            lemmatized_sentence.append(word)\n",
        "        else:        \n",
        "            #else use the tag to lemmatize the token\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "    return \" \".join(lemmatized_sentence)\n",
        "\n",
        "\n",
        "df1['tweet_processed'] = df1['tweet_processed'].apply(lambda x: lemmatize_sentence(x))"
      ],
      "metadata": {
        "id": "lebVgkEulBFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['tweet_intent'].unique()"
      ],
      "metadata": {
        "id": "DwvTcMpOEGLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing module\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import sklearn\n",
        "# Creating matrix of top 2500 tokens\n",
        "tfidf = TfidfVectorizer(max_features=2500)\n",
        "# tmp_df = tfidf.fit_transform(df.review_processed)\n",
        "# feature_names = tfidf.get_feature_names()\n",
        "# pd.DataFrame(tmp_df.toarray(), columns = feature_names).head() \n",
        "\n",
        "X = tfidf.fit_transform(df1.tweet_processed).toarray()\n",
        "y = pd.get_dummies(df1,columns=['tweet_intent'])\n",
        "featureNames = tfidf.get_feature_names()\n",
        "\n",
        "\n",
        "X, y\n",
        "\n",
        "\n",
        "# Splitting the dataset into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n"
      ],
      "metadata": {
        "id": "v9LXsaS66nNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "vRz3Yju_GaJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "0W59PxiJGcCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "mHIVlZhgGpZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "id": "1onX-ox6Jaen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for lr in [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.2,0.5,0.7,1]:\n",
        "  model = xgb.XGBRegressor(learning_rate = lr, n_estimators=100, verbosity = 0) # initialise the model\n",
        "  model.fit(X_train,y_train) #train the model\n",
        "  model.score(X_test, y_test) # scoring the model - r2 squared\n",
        "  print(\"Learning rate : \", lr, \" Train score : \", model.score(X_train,y_train), \" Cross-Val score : \", np.mean(cross_val_score(model, X_train, y_train, cv=10)))\n"
      ],
      "metadata": {
        "id": "jryiyrbCJ0jm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1dp130704PmyFtDu8TdvsVCFAE0qLd7uL",
      "authorship_tag": "ABX9TyOTBUwCaA3znK6QI8IA3iA+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}